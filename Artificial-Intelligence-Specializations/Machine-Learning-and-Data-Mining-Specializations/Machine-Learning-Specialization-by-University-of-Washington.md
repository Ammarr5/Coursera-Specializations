<img align="right" width="100" height="100" src="https://github.com/cs-MohamedAyman/Coursera-Specializations/blob/master/organizations-logos/university%20of%20washington.jpg">

# [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning) `100H`

## SKILLS YOU WILL GAIN
`data clustering algorithms` `machine learning` `classification algorithms` `decision tree` `python programming` `machine learning concepts` `deep learning` `linear regression` `ridge regression` `lasso (statistics)` `regression analysis` `logistic regression` `statistical classification` `k-means clustering` `k-d tree`

## About this Specialization
This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.

## Applied Learning Project
Learners will implement and apply predictive, classification, clustering, and information retrieval machine learning algorithms to real datasets throughout each course in the specialization. They will walk away with applied machine learning and Python programming experience.

<details>
	<summary>Specialization Details</summary>

- Do you have data and wonder what it can tell you? 
- Do you need a deeper understanding of the core ways in which machine learning can improve your business? 
- Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems?

- In the first course, you will get hands-on experience with machine learning from a series of practical case-studies. At the end of the first course you will have studied how to predict house prices based on house-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend products, and search for images. 
- Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains. This first course treats the machine learning method as a black box. Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms. Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications. Learning Outcomes: By the end of this course, you will be able to: -Identify potential applications of machine learning in practice. -Describe the core differences in analyses enabled by regression, classification, and clustering. -Select the appropriate machine learning task for a potential application. -Apply regression, classification, clustering, retrieval, recommender systems, and deep learning. -Represent your data as features to serve as input to machine learning models. -Assess the model quality in terms of relevant error metrics for each task. -Utilize a dataset to fit a model to analyze new data. -Build an end-to-end application that uses machine learning at its core. -Implement these techniques in Python.

- In our first case study (Predicting Housing Prices), predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...). This is just one of the many places where regression can be applied. Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression. 
- In this course, you will explore regularized linear regression models for the task of prediction and feature selection. You will be able to handle very large sets of features and select between models of various complexity. 
- You will also analyze the impact of aspects of your data -- such as outliers -- on your selected models and predictions. To fit these models, you will implement optimization algorithms that scale to large datasets. Learning Outcomes: By the end of this course, you will be able to: 
  - Describe the input and output of a regression model. 
  - Compare and contrast bias and variance when modeling data. 
  - Estimate model parameters using optimization algorithms. 
  - Tune parameters with cross validation. 
  - Analyze the performance of the model. 
  - Describe the notion of sparsity and how LASSO leads to sparse solutions. 
  - Deploy methods to select between models. 
  - Exploit the model to form predictions. 
  - Build a regression model to predict prices using a housing dataset. 
  - Implement these techniques in Python.

- In our second case study (Analyzing Sentiment & Loan Default Prediction) on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...). In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. 
- In this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks. You will become familiar with the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting. 
- In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent. You will implement these technique on real-world, large-scale machine learning tasks. You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier. This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data. We've also included optional content in every module, covering advanced topics for those who want to go even deeper! Learning Objectives: By the end of this course, you will be able to: 
  - Describe the input and output of a classification model. 
  - Tackle both binary and multiclass classification problems. 
  - Implement a logistic regression model for large-scale classification. 
  - Create a non-linear model using decision trees. 
  - Improve the performance of any model using boosting. 
  - Scale your methods with stochastic gradient ascent. 
  - Describe the underlying decision boundaries. 
  - Build a classification model to predict sentiment in a product review dataset. 
  - Analyze financial data to predict loan defaults. 
  - Use techniques for handling missing data. 
  - Evaluate your models using precision-recall metrics. 
  - Implement these techniques in Python (or in the language of your choice, though Python is highly recommended).

- A reader is interested in a specific news article and you want to find similar articles to recommend. What is the right notion of similarity? Moreover, what if there are millions of other documents? Each time you want to a retrieve a new document, do you need to search through all other documents? How do you group similar documents together? How do you discover new, emerging topics that the documents cover? 
- In this third case study (Finding Similar Documents), finding similar documents, you will examine similarity-based algorithms for retrieval. In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA). You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce. Learning Outcomes: By the end of this course, you will be able to: 
  - Create a document retrieval system using k-nearest neighbors. 
  - Identify various similarity metrics for text data. 
  - Reduce computations in k-nearest neighbor search by using KD-trees. 
  - Produce approximate nearest neighbors using locality sensitive hashing. 
  - Compare and contrast supervised and unsupervised learning tasks. 
  - Cluster documents by topic using k-means. 
  - Describe how to parallelize k-means using MapReduce. 
  - Examine probabilistic clustering approaches using mixtures models. 
  - Fit a mixture of Gaussian model using expectation maximization (EM). 
  - Perform mixed membership modeling using latent Dirichlet allocation (LDA). 
  - Describe the steps of a Gibbs sampler and how to use its output to draw inferences. 
  - Compare and contrast initialization techniques for non-convex optimization objectives. 
  - Implement these techniques in Python.

</details>

## There are 4 Courses in this Specialization

## Course 1: [Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations) `25H`

### Week 1: Welcome
```Machine learning is everywhere, but is often operating behind the scenes. This introduction to the specialization provides you with insights into the power of machine learning, and the multitude of intelligent applications you personally will be able to develop and deploy upon completion.We also discuss who we are, how we got here, and our view of the future of intelligent applications.```

<details>
      <summary>Week Details</summary>
<br>

- Why you should learn machine learning with us
  - Reading: Important Update regarding the Machine Learning Specialization
  - Reading: Slides presented in this module
  - Video: Welcome to this course and specialization
  - Video: Who we are
  - Video: Machine learning is changing the world
  - Video: Why a case study approach?
  - Video: Specialization overview
- Who this specialization is for and what you will be able to do
  - Video: How we got into ML
  - Video: Who is this specialization for?
  - Video: What you'll be able to do
  - Video: The capstone and an example intelligent application
  - Video: The future of intelligent applications
- Getting started with the tools for the course
  - Reading: Reading: Getting started with Python, IPython Notebook & GraphLab Create
  - Reading: Reading: where should my files go?
- Getting started with Python and the IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Starting an IPython Notebook
  - Video: Creating variables in Python
  - Video: Conditional statements and loops in Python
  - Video: Creating functions and lambdas in Python
- Getting started with SFrames for data engineering and analysis
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Starting GraphLab Create & loading an SFrame
  - Video: Canvas for data visualization
  - Video: Interacting with columns of an SFrame
  - Video: Using .apply() for data transformation
</details>

### Week 2: Regression: Predicting House Prices
```This week you will build your first intelligent application that makes predictions from data.We will explore this idea within the context of our first case study, predicting house prices, where you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...). This is just one of the many places where regression can be applied.Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.You will also examine how to analyze the performance of your predictive model and implement regression in practice using a Jupyter notebook.```

<details>
      <summary>Week Details</summary>
<br>

- Linear regression modeling
  - Reading: Slides presented in this module
  - Video: Predicting house prices: A case study in regression
  - Video: What is the goal and how might you naively address it?
  - Video: Linear Regression: A Model-Based Approach
  - Video: Adding higher order effects
- Evaluating regression models
  - Video: Evaluating overfitting via training/test split
  - Video: Training/test curves
  - Video: Adding other features
  - Video: Other regression examples
- Summary of regression
  - Video: Regression ML block diagram
  - Quiz: Regression
- Predicting house prices: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading & exploring house sale data
  - Video: Splitting the data into training and test sets
  - Video: Learning a simple regression model to predict house prices from house size
  - Video: Evaluating error (RMSE) of the simple model
  - Video: Visualizing predictions of simple model with Matplotlib
  - Video: Inspecting the model coefficients learned
  - Video: Exploring other features of the data
  - Video: Learning a model to predict house prices from more features
  - Video: Applying learned models to predict price of an average house
  - Video: Applying learned models to predict price of two fancy houses
- Programming assignment
  - Reading: Reading: Predicting house prices assignment
  - Quiz: Predicting house prices
</details>

### Week 3: Classification: Analyzing Sentiment
```How do you guess whether a person felt positively or negatively about an experience, just from a short review they wrote?In our second case study, analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).This task is an example of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification.You will analyze the accuracy of your classifier, implement an actual classifier in a Jupyter notebook, and take a first stab at a core piece of the intelligent application you will build and deploy in your capstone.```

<details>
      <summary>Week Details</summary>
<br>

- Classification modeling
  - Reading: Slides presented in this module
  - Video: Analyzing the sentiment of reviews: A case study in classification
  - Video: What is an intelligent restaurant review system?
  - Video: Examples of classification tasks
  - Video: Linear classifiers
  - Video: Decision boundaries
- Evaluating classification models
  - Video: Training and evaluating a classifier
  - Video: What's a good accuracy?
  - Video: False positives, false negatives, and confusion matrices
  - Video: Learning curves
  - Video: Class probabilities
- Summary of classification
  - Video: Classification ML block diagram
  - Quiz: Classification
- Analyzing sentiment: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading & exploring product review data
  - Video: Creating the word count vector
  - Video: Exploring the most popular product
  - Video: Defining which reviews have positive or negative sentiment
  - Video: Training a sentiment classifier
  - Video: Evaluating a classifier & the ROC curve
  - Video: Applying model to find most positive & negative reviews for a product
  - Video: Exploring the most positive & negative aspects of a product
- Programming assignment
  - Reading: Reading: Analyzing product sentiment assignment
  - Quiz: Analyzing product sentiment
</details>

### Week 4: Clustering and Similarity: Retrieving Documents
```A reader is interested in a specific news article and you want to find a similar articles to recommend. What is the right notion of similarity? How do I automatically search over documents to find the one that is most similar? How do I quantitatively represent the documents in the first place?In this third case study, retrieving documents, you will examine various document representations and an algorithm to retrieve the most similar subset. You will also consider structured representations of the documents that automatically group articles by similarity (e.g., document topic).You will actually build an intelligent document retrieval system for Wikipedia entries in an Jupyter notebook.```

<details>
      <summary>Week Details</summary>
<br>

- Algorithms for retrieval and measuring similarity of documents
  - Reading: Slides presented in this module
  - Video: Document retrieval: A case study in clustering and measuring similarity
  - Video: What is the document retrieval task?
  - Video: Word count representation for measuring similarity
  - Video: Prioritizing important words with tf-idf
  - Video: Calculating tf-idf vectors
  - Video: Retrieving similar documents using nearest neighbor search
- Clustering models and algorithms
  - Video: Clustering documents task overview
  - Video: Clustering documents: An unsupervised learning task
  - Video: k-means: A clustering algorithm
  - Video: Other examples of clustering
- Summary of clustering and similarity
  - Video: Clustering and similarity ML block diagram
  - Quiz: Clustering and Similarity
- Document retrieval: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading & exploring Wikipedia data
  - Video: Exploring word counts
  - Video: Computing & exploring TF-IDFs
  - Video: Computing distances between Wikipedia articles
  - Video: Building & exploring a nearest neighbors model for Wikipedia articles
  - Video: Examples of document retrieval in action
- Programming assignment
  - Reading: Reading: Retrieving Wikipedia articles assignment
  - Quiz: Retrieving Wikipedia articles
</details>

### Week 5: Recommending Products
```Ever wonder how Amazon forms its personalized product recommendations? How Netflix suggests movies to watch? How Pandora selects the next song to stream? How Facebook or LinkedIn finds people you might connect with? Underlying all of these technologies for personalized content is something called collaborative filtering. You will learn how to build such a recommender system using a variety of techniques, and explore their tradeoffs. One method we examine is matrix factorization, which learns features of users and products to form recommendations. In a Jupyter notebook, you will use these techniques to build a real song recommender system.```

<details>
      <summary>Week Details</summary>
<br>

- Recommender systems
  - Reading: Slides presented in this module
  - Video: Recommender systems overview
  - Video: Where we see recommender systems in action
  - Video: Building a recommender system via classification
- Co-occurrence matrices for collaborative filtering
  - Video: Collaborative filtering: People who bought this also bought...
  - Video: Effect of popular items
  - Video: Normalizing co-occurrence matrices and leveraging purchase histories
- Matrix factorization
  - Video: The matrix completion task
  - Video: Recommendations from known user/item features
  - Video: Predictions in matrix form
  - Video: Discovering hidden structure by matrix factorization
  - Video: Bringing it all together: Featurized matrix factorization
- Performance metrics for recommender systems
  - Video: A performance metric for recommender systems
  - Video: Optimal recommenders
  - Video: Precision-recall curves
- Summary of recommender systems
  - Video: Recommender systems ML block diagram
  - Quiz: Recommender Systems
- Song recommender: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading and exploring song data
  - Video: Creating & evaluating a popularity-based song recommender
  - Video: Creating & evaluating a personalized song recommender
  - Video: Using precision-recall to compare recommender models
- Programming assignment
  - Reading: Reading: Recommending songs assignment
  - Quiz: Recommending songs
</details>

### Week 6: Deep Learning: Searching for Images & Closing Remarks
```You’ve probably heard that Deep Learning is making news across the world as one of the most promising techniques in machine learning. Every industry is dedicating resources to unlock the deep learning potential, including for tasks such as image tagging, object recognition, speech recognition, and text analysis.In our final case study, searching for images, you will learn how layers of neural networks provide very descriptive (non-linear) features that provide impressive performance in image classification and retrieval tasks. You will then construct deep features, a transfer learning technique that allows you to use deep learning very easily, even when you have little data to train the model.Using iPhython notebooks, you will build an image classifier and an intelligent image retrieval system with deep learning.```
```In the conclusion of the course, we will describe the final stage in turning our machine learning tools into a service: deployment.We will also discuss some open challenges that the field of machine learning still faces, and where we think machine learning is heading. We conclude with an overview of what's in store for you in the rest of the specialization, and the amazing intelligent applications that are ahead for us as we evolve machine learning.```

<details>
      <summary>Week Details</summary>
<br>

- Neural networks: Learning very non-linear features
  - Reading: Slides presented in this module
  - Video: Searching for images: A case study in deep learning
  - Video: What is a visual product recommender?
  - Video: Learning very non-linear features with neural networks
- Deep learning & deep features
  - Video: Application of deep learning to computer vision
  - Video: Deep learning performance
  - Video: Demo of deep learning model on ImageNet data
  - Video: Other examples of deep learning in computer vision
  - Video: Challenges of deep learning
  - Video: Deep Features
- Summary of deep learning
  - Video: Deep learning ML block diagram
  - Quiz: Deep Learning
- Deep features for image classification: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading image data
  - Video: Training & evaluating a classifier using raw image pixels
  - Video: Training & evaluating a classifier using deep features
- Deep features for image retrieval: IPython Notebook
  - Reading: Download the IPython Notebook used in this lesson to follow along
  - Video: Loading image data
  - Video: Creating a nearest neighbors model for image retrieval
  - Video: Querying the nearest neighbors model to retrieve images
  - Video: Querying for the most similar images for car image
  - Video: Displaying other example image retrievals with a Python lambda
- Programming assignment
  - Reading: Reading: Deep features for image retrieval assignment
  - Quiz: Deep features for image retrieval

- Deploying machine learning as a service
  - Reading: Slides presented in this module
  - Video: You've made it!
  - Video: Deploying an ML service
  - Video: What happens after deployment?
- Machine learning challenges and future directions
  - Video: Open challenges in ML
  - Video: Where is ML going?
  - Video: What's ahead in the specialization
  - Video: Thank you!
</details>

## Course 2: [Machine Learning: Regression](https://www.coursera.org/learn/ml-regression) `25H`

### Week 1: Welcome & Simple Linear Regression
```Regression is one of the most important and broadly used machine learning and statistics tools out there. It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response. Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.```

```Our course starts from the most basic regression model: Just fitting a line to data. This simple model for forming predictions from a single, univariate feature of the data is appropriately called "simple linear regression". In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent. Based on this fitted function, you will interpret the estimated model parameters and form predictions. You will also analyze the sensitivity of your fit to outlying observations. You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house.```

<details>
      <summary>Week Details</summary>
<br>

- What is this course about?
  - Reading: Important Update regarding the Machine Learning Specialization
  - Reading: Slides presented in this module
  - Video: Welcome!
  - Video: What is the course about?
  - Video: Outlining the first half of the course
  - Video: Outlining the second half of the course
  - Video: Assumed background
  - Reading: Reading: Software tools you'll need
- Regression fundamentals
  - Reading: Slides presented in this module
  - Video: A case study in predicting house prices
  - Video: Regression fundamentals: data & model
  - Video: Regression fundamentals: the task
  - Video: Regression ML block diagram
- The simple linear regression model, its use, and interpretation
  - Video: The simple linear regression model
  - Video: The cost of using a given line
  - Video: Using the fitted line
  - Video: Interpreting the fitted line
- An aside on optimization: one dimensional objectives
  - Video: Defining our least squares optimization objective
  - Video: Finding maxima or minima analytically
  - Video: Maximizing a 1d function: a worked example
  - Video: Finding the max via hill climbing
  - Video: Finding the min via hill descent
  - Video: Choosing stepsize and convergence criteria
- An aside on optimization: multidimensional objectives
  - Video: Gradients: derivatives in multiple dimensions
  - Video: Gradient descent: multidimensional hill descent
- Finding the least squares line
  - Video: Computing the gradient of RSS
  - Video: Approach 1: closed-form solution
  - Reading: Optional reading: worked-out example for closed-form solution
  - Video: Approach 2: gradient descent
  - Reading: Optional reading: worked-out example for gradient descent
  - Video: Comparing the approaches
- Discussion and summary of simple linear regression
  - Reading: Download notebooks to follow along
  - Video: Influence of high leverage points: exploring the data
  - Video: Influence of high leverage points: removing Center City
  - Video: Influence of high leverage points: removing high-end towns
  - Video: Asymmetric cost functions
  - Video: A brief recap
  - Quiz: Simple Linear Regression
- Programming assignment
  - Reading: Reading: Fitting a simple linear regression model on housing data
  - Quiz: Fitting a simple linear regression model on housing data
</details>

### Week 2: Multiple Regression
```The next step in moving beyond simple linear regression is to consider "multiple regression" where multiple features of the data are used to form predictions.  More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., 'square feet') and the observed response (like 'house sales price'). This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value. You will also learn how to incorporate multiple input variables (e.g., 'square feet', '# bedrooms', '# bathrooms'). You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple "features". Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. Here, you will also implement a gradient descent algorithm for fitting a multiple regression model.```

<details>
      <summary>Week Details</summary>
<br>

- Multiple features of one input
  - Reading: Slides presented in this module
  - Video: Multiple regression intro
  - Video: Polynomial regression
  - Video: Modeling seasonality
  - Video: Where we see seasonality
  - Video: Regression with general features of 1 input
- Incorporating multiple inputs
  - Video: Motivating the use of multiple inputs
  - Video: Defining notation
  - Video: Regression with features of multiple inputs
  - Video: Interpreting the multiple regression fit
- Setting the stage for computing the least squares fit
  - Reading: Optional reading: review of matrix algebra
  - Video: Rewriting the single observation model in vector notation
  - Video: Rewriting the model for all observations in matrix notation
  - Video: Computing the cost of a D-dimensional curve
- Computing the least squares D-dimensional curve
  - Video: Computing the gradient of RSS
  - Video: Approach 1: closed-form solution
  - Video: Discussing the closed-form solution
  - Video: Approach 2: gradient descent
  - Video: Feature-by-feature update
  - Video: Algorithmic summary of gradient descent approach
- Summarizing multiple regression
  - Video: A brief recap
  - Quiz: Multiple Regression
- Programming assignment 1
  - Reading: Reading: Exploring different multiple regression models for house price prediction
  - Quiz: Exploring different multiple regression models for house price prediction
- Programming assignment 2
  - Reading: Numpy tutorial
  - Reading: Reading: Implementing gradient descent for multiple regression
  - Quiz: Implementing gradient descent for multiple regression
</details>

### Week 3: Assessing Performance
```Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data. You are also ready to select amongst possible models to choose the best performing.  This module is all about these important topics of model selection and assessment. You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the "loss" of your predictions, and use this to define training, test, and generalization error. For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance. This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning. Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course.```

<details>
      <summary>Week Details</summary>
<br>

- Defining how we assess performance
  - Reading: Slides presented in this module
  - Video: Assessing performance intro
  - Video: What do we mean by "loss"?
- 3 measures of loss and their trends with model complexity
  - Video: Training error: assessing loss on the training set
  - Video: Generalization error: what we really want
  - Video: Test error: what we can actually compute
  - Video: Defining overfitting
  - Video: Training/test split
- 3 sources of error and the bias-variance tradeoff
  - Video: Irreducible error and bias
  - Video: Variance and the bias-variance tradeoff
  - Video: Error vs. amount of data
- OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error
  - Video: Formally defining the 3 sources of error
  - Video: Formally deriving why 3 sources of error
- Putting the pieces together
  - Video: Training/validation/test split for model selection, fitting, and assessment
  - Video: A brief recap
  - Quiz: Assessing Performance
- Programming assignment
  - Reading: Reading: Exploring the bias-variance tradeoff
  - Quiz: Exploring the bias-variance tradeoff
</details>

### Week 4: Ridge Regression
```You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data. In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue. This method is called "ridge regression". You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions. To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective. You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression. To select the strength of the bias away from overfitting, you will explore a general-purpose method called "cross validation". You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant.```

<details>
      <summary>Week Details</summary>
<br>

- Characteristics of overfit models
  - Reading: Slides presented in this module
  - Video: Symptoms of overfitting in polynomial regression
  - Reading: Download the notebook and follow along
  - Video: Overfitting demo
  - Video: Overfitting for more general multiple regression models
- The ridge objective
  - Video: Balancing fit and magnitude of coefficients
  - Video: The resulting ridge objective and its extreme solutions
  - Video: How ridge regression balances bias and variance
  - Reading: Download the notebook and follow along
  - Video: Ridge regression demo
  - Video: The ridge coefficient path
- Optimizing the ridge objective
  - Video: Computing the gradient of the ridge objective
  - Video: Approach 1: closed-form solution
  - Video: Discussing the closed-form solution
  - Video: Approach 2: gradient descent
- Tying up the loose ends
  - Video: Selecting tuning parameters via cross validation
  - Video: K-fold cross validation
  - Video: How to handle the intercept
  - Video: A brief recap
  - Quiz: Ridge Regression
- Programming Assignment 1
  - Reading: Reading: Observing effects of L2 penalty in polynomial regression
  - Quiz: Observing effects of L2 penalty in polynomial regression
- Programming Assignment 2
  - Reading: Reading: Implementing ridge regression via gradient descent
  - Quiz: Implementing ridge regression via gradient descent
</details>

### Week 5: Feature Selection & Lasso
```A fundamental machine learning task is to select amongst a set of features to include in a model. In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions.  To start, you will examine methods that search over an enumeration of models including different subsets of features. You will analyze both exhaustive search and greedy algorithms. Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge. This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning.```

<details>
      <summary>Week Details</summary>
<br>

- Feature selection via explicit model enumeration
  - Reading: Slides presented in this module
  - Video: The feature selection task
  - Video: All subsets
  - Video: Complexity of all subsets
  - Video: Greedy algorithms
  - Video: Complexity of the greedy forward stepwise algorithm
- Feature selection implicitly via regularized regression
  - Video: Can we use regularization for feature selection?
  - Video: Thresholding ridge coefficients?
  - Video: The lasso objective and its coefficient path
- Geometric intuition for sparsity of lasso solutions
  - Video: Visualizing the ridge cost
  - Video: Visualizing the ridge solution
  - Video: Visualizing the lasso cost and solution
  - Reading: Download the notebook and follow along
  - Video: Lasso demo
- Setting the stage for solving the lasso
  - Video: What makes the lasso objective different
  - Video: Coordinate descent
  - Video: Normalizing features
  - Video: Coordinate descent for least squares regression (normalized features)
- Optimizing the lasso objective
  - Video: Coordinate descent for lasso (normalized features)
  - Video: Assessing convergence and other lasso solvers
  - Video: Coordinate descent for lasso (unnormalized features)
- OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update
  - Video: Deriving the lasso coordinate descent update
- Tying up loose ends
  - Video: Choosing the penalty strength and other practical issues with lasso
  - Video: A brief recap
  - Quiz: Feature Selection and Lasso
- Programming Assignment 1
  - Reading: Reading: Using LASSO to select features
  - Quiz: Using LASSO to select features
- Programming Assignment 2
  - Reading: Reading: Implementing LASSO using coordinate descent
  - Quiz: Implementing LASSO using coordinate descent
</details>

### Week 6: Nearest Neighbors & Kernel Regression & Closing Remarks
```Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset. In this module, we instead turn our attention to a class of "nonparametric" methods. These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.  We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set. This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors. Building on this idea, we turn to kernel regression. Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point. You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle. You will also implement these techniques and observe their practical behavior.```

<details>
      <summary>Week Details</summary>
<br>

- Motivating local fits
  - Reading: Slides presented in this module
  - Video: Limitations of parametric regression
- Nearest neighbor regression
  - Video: 1-Nearest neighbor regression approach
  - Video: Distance metrics
  - Video: 1-Nearest neighbor algorithm
- k-Nearest neighbors and weighted k-nearest neighbors
  - Video: k-Nearest neighbors regression
  - Video: k-Nearest neighbors in practice
  - Video: Weighted k-nearest neighbors
- Kernel regression
  - Video: From weighted k-NN to kernel regression
  - Video: Global fits of parametric models vs. local fits of kernel regression
- k-NN and kernel regression wrapup
  - Video: Performance of NN as amount of data grows
  - Video: Issues with high-dimensions, data scarcity, and computational complexity
  - Video: k-NN for classification
  - Video: A brief recap
  - Quiz: Nearest Neighbors & Kernel Regression
- Programming Assignment
  - Reading: Reading: Predicting house prices using k-nearest neighbors regression
  - Quiz: Predicting house prices using k-nearest neighbors regression
- What we've learned
  - Reading: Slides presented in this module
  - Video: Simple and multiple regression
  - Video: Assessing performance and ridge regression
  - Video: Feature selection, lasso, and nearest neighbor regression
- Summary and what's ahead in the specialization
  - Video: What we covered and what we didn't cover
  - Video: Thank you!
</details>

## Course 3: [Machine Learning: Classification](https://www.coursera.org/learn/ml-classification) `25H`

### Week 1: Welcome! & Linear Classifiers & Logistic Regression
```Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.```

```Linear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task.```

<details>
      <summary>Week Details</summary>
<br>

- Welcome to the course
  - Reading: Important Update regarding the Machine Learning Specialization
  - Reading: Slides presented in this module
  - Video: Welcome to the classification course, a part of the Machine Learning Specialization
  - Video: What is this course about?
  - Video: Impact of classification
- Course overview and details
  - Video: Course overview
  - Video: Outline of first half of course
  - Video: Outline of second half of course
  - Video: Assumed background
  - Video: Let's get started!
  - Reading: Reading: Software tools you'll need
- Linear classifiers
  - Reading: Slides presented in this module
  - Video: Linear classifiers: A motivating example
  - Video: Intuition behind linear classifiers
  - Video: Decision boundaries
  - Video: Linear classifier model
  - Video: Effect of coefficient values on decision boundary
  - Video: Using features of the inputs
- Class probabilities
  - Video: Predicting class probabilities
  - Video: Review of basics of probabilities
  - Video: Review of basics of conditional probabilities
  - Video: Using probabilities in classification
- Logistic regression
  - Video: Predicting class probabilities with (generalized) linear models
  - Video: The sigmoid (or logistic) link function
  - Video: Logistic regression model
  - Video: Effect of coefficient values on predicted probabilities
  - Video: Overview of learning logistic regression models
- Practical issues for classification
  - Video: Encoding categorical inputs
  - Video: Multiclass classification with 1 versus all
- Summarizing linear classifiers & logistic regression
  - Video: Recap of logistic regression classifier
  - Quiz: Linear Classifiers & Logistic Regression
- Programming Assignment
  - Reading: Predicting sentiment from product reviews
  - Quiz: Predicting sentiment from product reviews
</details>

### Week 2: Learning Linear Classifiers & Overfitting & Regularization in Logistic Regression
```Once familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression. You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier.```

```As we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers' outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data.```

<details>
      <summary>Week Details</summary>
<br>

- Maximum likelihood estimation
  - Reading: Slides presented in this module
  - Video: Goal: Learning parameters of logistic regression
  - Video: Intuition behind maximum likelihood estimation
  - Video: Data likelihood
  - Video: Finding best linear classifier with gradient ascent
- Gradient ascent algorithm for learning logistic regression classifier
  - Video: Review of gradient ascent
  - Video: Learning algorithm for logistic regression
  - Video: Example of computing derivative for logistic regression
  - Video: Interpreting derivative for logistic regression
  - Video: Summary of gradient ascent for logistic regression
- Choosing step size for gradient ascent/descent
  - Video: Choosing step size
  - Video: Careful with step sizes that are too large
  - Video: Rule of thumb for choosing step size
- (VERY OPTIONAL LESSON) Deriving gradient of logistic regression
  - Video: (VERY OPTIONAL) Deriving gradient of logistic regression: Log trick
  - Video: (VERY OPTIONAL) Expressing the log-likelihood
  - Video: (VERY OPTIONAL) Deriving probability y=-1 given x
  - Video: (VERY OPTIONAL) Rewriting the log likelihood into a simpler form
  - Video: (VERY OPTIONAL) Deriving gradient of log likelihood
- Summarizing learning linear classifiers
  - Video: Recap of learning logistic regression classifiers
  - Quiz: Learning Linear Classifiers
- Programming Assignment
  - Reading: Implementing logistic regression from scratch
  - Quiz: Implementing logistic regression from scratch
- Overfitting in classification
  - Reading: Slides presented in this module
  - Video: Evaluating a classifier
  - Video: Review of overfitting in regression
  - Video: Overfitting in classification
  - Video: Visualizing overfitting with high-degree polynomial features
- Overconfident predictions due to overfitting
  - Video: Overfitting in classifiers leads to overconfident predictions
  - Video: Visualizing overconfident predictions
  - Video: (OPTIONAL) Another perspecting on overfitting in logistic regression
- L2 regularized logistic regression
  - Video: Penalizing large coefficients to mitigate overfitting
  - Video: L2 regularized logistic regression
  - Video: Visualizing effect of L2 regularization in logistic regression
  - Video: Learning L2 regularized logistic regression with gradient ascent
- Sparse logistic regression
  - Video: Sparse logistic regression with L1 regularization
- Summarizing overfitting & regularization in logistic regression
  - Video: Recap of overfitting & regularization in logistic regression
  - Quiz: Overfitting & Regularization in Logistic Regression
- Programming Assignment
  - Reading: Logistic Regression with L2 regularization
  - Quiz: Logistic Regression with L2 regularization
</details>

### Week 3: Decision Trees
```Along with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. In this module, you will become familiar with the core decision trees representation. You will then design a simple, recursive greedy algorithm to learn decision trees from data. Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data.```

<details>
      <summary>Week Details</summary>
<br>

- Intuition behind decision trees
  - Reading: Slides presented in this module
  - Video: Predicting loan defaults with decision trees
  - Video: Intuition behind decision trees
  - Video: Task of learning decision trees from data
- Learning decision trees
  - Video: Recursive greedy algorithm
  - Video: Learning a decision stump
  - Video: Selecting best feature to split on
  - Video: When to stop recursing
- Using the learned decision tree
  - Video: Making predictions with decision trees
  - Video: Multiclass classification with decision trees
- Learning decision trees with continuous inputs
  - Video: Threshold splits for continuous inputs
  - Video: (OPTIONAL) Picking the best threshold to split on
  - Video: Visualizing decision boundaries
- Summarizing decision trees
  - Video: Recap of decision trees
  - Quiz: Decision Trees
- Programming Assignment 1
  - Reading: Identifying safe loans with decision trees
  - Quiz: Identifying safe loans with decision trees
- Programming Assignment 2
  - Reading: Implementing binary decision trees
  - Quiz: Implementing binary decision trees
</details>

### Week 4: Preventing Overfitting in Decision Trees & Handling Missing Data
```Out of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam's razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set.```

```Real-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation.```

<details>
      <summary>Week Details</summary>
<br>

- Overfitting in decision trees
  - Reading: Slides presented in this module
  - Video: A review of overfitting
  - Video: Overfitting in decision trees
- Early stopping to avoid overfitting
  - Video: Principle of Occam's razor: Learning simpler decision trees
  - Video: Early stopping in learning decision trees
- (OPTIONAL LESSON) Pruning decision trees
  - Video: (OPTIONAL) Motivating pruning
  - Video: (OPTIONAL) Pruning decision trees to avoid overfitting
  - Video: (OPTIONAL) Tree pruning algorithm
- Summarizing preventing overfitting in decision trees
  - Video: Recap of overfitting and regularization in decision trees
  - Quiz: Preventing Overfitting in Decision Trees
- Programming Assignment
  - Reading: Decision Trees in Practice
  - Quiz: Decision Trees in Practice
- Basic strategies for handling missing data
  - Reading: Slides presented in this module
  - Video: Challenge of missing data
  - Video: Strategy 1: Purification by skipping missing data
  - Video: Strategy 2: Purification by imputing missing data
- Strategy 3: Modify learning algorithm to explicitly handle missing data
  - Video: Modifying decision trees to handle missing data
  - Video: Feature split selection with missing data
- Summarizing handling missing data
  - Video: Recap of handling missing data
  - Quiz: Handling Missing Data
</details>

### Week 5: Boosting
```One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it's used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data.```

<details>
      <summary>Week Details</summary>
<br>

- The amazing idea of boosting a classifier
  - Reading: Slides presented in this module
  - Video: The boosting question
  - Video: Ensemble classifiers
  - Video: Boosting
- AdaBoost
  - Video: AdaBoost overview
  - Video: Weighted error
  - Video: Computing coefficient of each ensemble component
  - Video: Reweighing data to focus on mistakes
  - Video: Normalizing weights
- Applying AdaBoost
  - Video: Example of AdaBoost in action
  - Video: Learning boosted decision stumps with AdaBoost
- Programming Assignment 1
  - Reading: Exploring Ensemble Methods
  - Quiz: Exploring Ensemble Methods
- Convergence and overfitting in boosting
  - Video: The Boosting Theorem
  - Video: Overfitting in boosting
- Summarizing boosting
  - Video: Ensemble methods, impact of boosting & quick recap
  - Quiz: Boosting
- Programming Assignment 2
  - Reading: Boosting a decision stump
  - Quiz: Boosting a decision stump
</details>

### Week 6: Precision-Recall
```In many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data.```

<details>
      <summary>Week Details</summary>
<br>

- Why use precision & recall as quality metrics
  - Reading: Slides presented in this module
  - Video: Case-study where accuracy is not best metric for classification
  - Video: What is good performance for a classifier?
- Precision & recall explained
  - Video: Precision: Fraction of positive predictions that are actually positive
  - Video: Recall: Fraction of positive data predicted to be positive
- The precision-recall tradeoff
  - Video: Precision-recall extremes
  - Video: Trading off precision and recall
  - Video: Precision-recall curve
- Summarizing precision-recall
  - Video: Recap of precision-recall
  - Quiz: Precision-Recall
- Programming Assignment
  - Reading: Exploring precision and recall
  - Quiz: Exploring precision and recall
</details>

### Week 7: Scaling to Huge Datasets & Online Learning
```With the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called "Big Data". Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data.```

<details>
      <summary>Week Details</summary>
<br>

- Scaling ML to huge datasets
  - Reading: Slides presented in this module
  - Video: Gradient ascent won't scale to today's huge datasets
  - Video: Timeline of scalable machine learning & stochastic gradient
- Scaling ML with stochastic gradient
  - Video: Why gradient ascent won't scale
  - Video: Stochastic gradient: Learning one data point at a time
  - Video: Comparing gradient to stochastic gradient
- Understanding why stochastic gradient works
  - Video: Why would stochastic gradient ever work?
  - Video: Convergence paths
- Stochastic gradient: Practical tricks
  - Video: Shuffle data before running stochastic gradient
  - Video: Choosing step size
  - Video: Don't trust last coefficients
  - Video: (OPTIONAL) Learning from batches of data
  - Video: (OPTIONAL) Measuring convergence
  - Video: (OPTIONAL) Adding regularization
- Online learning: Fitting models from streaming data
  - Video: The online learning task
  - Video: Using stochastic gradient for online learning
- Summarizing scaling to huge datasets & online learning
  - Video: Scaling to huge datasets through parallelization & module recap
  - Quiz: Scaling to Huge Datasets & Online Learning
- Programming Assignment
  - Reading: Training Logistic Regression via Stochastic Gradient Ascent
  - Quiz: Training Logistic Regression via Stochastic Gradient Ascent
</details>

## Course 4: [Machine Learning: Clustering & Retrieval](https://www.coursera.org/learn/ml-clustering-and-retrieval) `25H`

### Week 1: Welcome
```Clustering and retrieval are some of the most high-impact machine learning tools out there. Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform. Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.```

<details>
      <summary>Week Details</summary>
<br>

- What is this course about?
  - Reading: Important Update regarding the Machine Learning Specialization
  - Reading: Slides presented in this module
  - Video: Welcome and introduction to clustering and retrieval tasks
  - Video: Course overview
  - Video: Module-by-module topics covered
  - Video: Assumed background
  - Reading: Software tools you'll need for this course
  - Reading: A big week ahead!
</details>

### Week 2: Nearest Neighbor Search
```We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading. We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses. However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints. You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces. You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced.```

<details>
      <summary>Week Details</summary>
<br>

- Introduction to nearest neighbor search and algorithms
  - Reading: Slides presented in this module
  - Video: Retrieval as k-nearest neighbor search
  - Video: 1-NN algorithm
  - Video: k-NN algorithm
- The importance of data representations and distance metrics
  - Video: Document representation
  - Video: Distance metrics: Euclidean and scaled Euclidean
  - Video: Writing (scaled) Euclidean distance using (weighted) inner products
  - Video: Distance metrics: Cosine similarity
  - Video: To normalize or not and other distance considerations
  - Quiz: Representations and metrics
- Programming Assignment 1
  - Reading: Choosing features and metrics for nearest neighbor search
  - Quiz: Choosing features and metrics for nearest neighbor search
- Scaling up k-NN search using KD-trees
  - Video: Complexity of brute force search
  - Video: KD-tree representation
  - Video: NN search with KD-trees
  - Video: Complexity of NN search with KD-trees
  - Video: Visualizing scaling behavior of KD-trees
  - Video: Approximate k-NN search using KD-trees
  - Reading: (OPTIONAL) A worked-out example for KD-trees
  - Quiz: KD-trees
- Locality sensitive hashing for approximate NN search
  - Video: Limitations of KD-trees
  - Video: LSH as an alternative to KD-trees
  - Video: Using random lines to partition points
  - Video: Defining more bins
  - Video: Searching neighboring bins
  - Video: LSH in higher dimensions
  - Video: (OPTIONAL) Improving efficiency through multiple tables
  - Quiz: Locality Sensitive Hashing
- Programming Assignment 2
  - Reading: Implementing Locality Sensitive Hashing from scratch
  - Quiz: Implementing Locality Sensitive Hashing from scratch
- Summarizing nearest neighbor search
  - Video: A brief recap
</details>

### Week 3: Clustering with k-means
```In clustering, our goal is to group the datapoints in our dataset into disjoint sets. Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by "topic". These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like "Science", "World News", etc. Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset. The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there. To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework. You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned.```

<details>
      <summary>Week Details</summary>
<br>

- Introduction to clustering
  - Reading: Slides presented in this module
  - Video: The goal of clustering
  - Video: An unsupervised task
  - Video: Hope for unsupervised learning, and some challenge cases
- Clustering via k-means
  - Video: The k-means algorithm
  - Video: k-means as coordinate descent
  - Video: Smart initialization via k-means++
  - Video: Assessing the quality and choosing the number of clusters
  - Quiz: k-means
- Programming Assignment
  - Reading: Clustering text data with k-means
  - Quiz: Clustering text data with K-means
- MapReduce for scaling k-means
  - Video: Motivating MapReduce
  - Video: The general MapReduce abstraction
  - Video: MapReduce execution overview and combiners
  - Video: MapReduce for k-means
  - Quiz: MapReduce for k-means
- Summarizing clustering with k-means
  - Video: Other applications of clustering
  - Video: A brief recap
</details>

### Week 4: Mixture Models
```In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information. In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a "cluster" and (2) accounts for uncertainty in assignments of datapoints to clusters via "soft assignments". You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters. To gain intuition, you will first consider a visually appealing image clustering task. You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered.```

<details>
      <summary>Week Details</summary>
<br>

- Motivating and setting the foundation for mixture models
  - Reading: Slides presented in this module
  - Video: Motiving probabilistic clustering models
  - Video: Aggregating over unknown classes in an image dataset
  - Video: Univariate Gaussian distributions
  - Video: Bivariate and multivariate Gaussians
- Mixtures of Gaussians for clustering
  - Video: Mixture of Gaussians
  - Video: Interpreting the mixture of Gaussian terms
  - Video: Scaling mixtures of Gaussians for document clustering
- Expectation Maximization (EM) building blocks
  - Video: Computing soft assignments from known cluster parameters
  - Video: (OPTIONAL) Responsibilities as Bayes' rule
  - Video: Estimating cluster parameters from known cluster assignments
  - Video: Estimating cluster parameters from soft assignments
- The EM algorithm
  - Video: EM iterates in equations and pictures
  - Video: Convergence, initialization, and overfitting of EM
  - Video: Relationship to k-means
  - Reading: (OPTIONAL) A worked-out example for EM
  - Quiz: EM for Gaussian mixtures
- Summarizing mixture models
  - Video: A brief recap
- Programming Assignment 1
  - Reading: Implementing EM for Gaussian mixtures
  - Quiz: Implementing EM for Gaussian mixtures
- Programming Assignment 2
  - Reading: Clustering text data with Gaussian mixtures
  - Quiz: Clustering text data with Gaussian mixtures
</details>

### Week 5: Mixed Membership Modeling via Latent Dirichlet Allocation
```The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic. But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics. In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis. You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features. The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling. You will be able to implement a Gibbs sampler for LDA by the end of the module.```

<details>
      <summary>Week Details</summary>
<br>

- Introduction to latent Dirichlet allocation
  - Reading: Slides presented in this module
  - Video: Mixed membership models for documents
  - Video: An alternative document clustering model
  - Video: Components of latent Dirichlet allocation model
  - Video: Goal of LDA inference
  - Quiz: Latent Dirichlet Allocation
- Bayesian inference via Gibbs sampling
  - Video: The need for Bayesian inference
  - Video: Gibbs sampling from 10,000 feet
  - Video: A standard Gibbs sampler for LDA
- Collapsed Gibbs sampling for LDA
  - Video: What is collapsed Gibbs sampling?
  - Video: A worked example for LDA: Initial setup
  - Video: A worked example for LDA: Deriving the resampling distribution
  - Video: Using the output of collapsed Gibbs sampling
- Summarizing latent Dirichlet allocation
  - Video: A brief recap
  - Quiz: Learning LDA model via Gibbs sampling
- Programming Assignment
  - Reading: Modeling text topics with Latent Dirichlet Allocation
  - Quiz: Modeling text topics with Latent Dirichlet Allocation
</details>

### Week 6: Hierarchical Clustering & Closing Remarks
```In the conclusion of the course, we will recap what we have covered. This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset. Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series. We then briefly outline some important clustering and retrieval ideas that we did not cover in this course. We conclude with an overview of what's in store for you in the rest of the specialization.```

<details>
      <summary>Week Details</summary>
<br>

- What we've learned
  - Reading: Slides presented in this module
  - Video: Module 1 recap
  - Video: Module 2 recap
  - Video: Module 3 recap
  - Video: Module 4 recap
- Hierarchical clustering and clustering for time series segmentation
  - Video: Why hierarchical clustering?
  - Video: Divisive clustering
  - Video: Agglomerative clustering
  - Video: The dendrogram
  - Video: Agglomerative clustering details
  - Video: Hidden Markov models
- Programming Assignment
  - Reading: Modeling text data with a hierarchy of clusters
  - Quiz: Modeling text data with a hierarchy of clusters
- Summary and what's ahead in the specialization
  - Video: What we didn't cover
  - Video: Thank you!
</details>
